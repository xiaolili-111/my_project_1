{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-30T07:27:34.477922Z",
     "start_time": "2025-11-30T07:27:05.942034Z"
    }
   },
   "source": [
    "import torch\n",
    "from modelscope import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 15:27:06,085 - modelscope - INFO - PyTorch version 2.9.0+cu128 Found.\n",
      "2025-11-30 15:27:06,088 - modelscope - INFO - TensorFlow version 2.20.0 Found.\n",
      "2025-11-30 15:27:06,089 - modelscope - INFO - Loading ast index from C:\\Users\\22745\\.cache\\modelscope\\ast_indexer\n",
      "2025-11-30 15:27:06,263 - modelscope - INFO - Loading done! Current index file version is 1.11.0, with md5 41bcbf5c50ca827b348f5d44b36ba914 and a total number of 953 components indexed\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T07:27:34.484184Z",
     "start_time": "2025-11-30T07:27:34.479195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 路径设置\n",
    "model_path = \"Qwen/Qwen2.5-0.5B\"        # 原始底座模型\n",
    "lora_path = \"./qwen2.5-0.5b-otaku-lora\" # 你训练好的 LoRA 保存路径\n",
    "\n",
    "print(\"正在加载模型...\")"
   ],
   "id": "46683287dfe0cc45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型...\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T07:27:35.681115Z",
     "start_time": "2025-11-30T07:27:34.487538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. 加载 Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)"
   ],
   "id": "616ab329cf9a19d0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T07:27:39.695901Z",
     "start_time": "2025-11-30T07:27:35.682748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. 加载底座模型 (使用 bfloat16 或 float16)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16, # 如果显卡比较旧(如1080Ti/2080Ti)用 float16，新显卡(3090/4090/A100)可以用 bfloat16\n",
    "    trust_remote_code=True\n",
    ")"
   ],
   "id": "49676e25c85b7ccd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T07:27:39.848023Z",
     "start_time": "2025-11-30T07:27:39.706868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4. 加载 LoRA 适配器并与底座合并\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "model.eval() # 切换到推理模式"
   ],
   "id": "3dd21902445a2a7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 896)\n",
       "        (layers): ModuleList(\n",
       "          (0-23): 24 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=896, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=896, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=128, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=896, out_features=896, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=896, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=896, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "              (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "              (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T07:27:39.855459Z",
     "start_time": "2025-11-30T07:27:39.851134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"模型加载完成！输入 'exit' 退出对话。\")\n",
    "print(\"-\" * 30)"
   ],
   "id": "65e4d0a441a29857",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！输入 'exit' 退出对话。\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T07:27:39.881192Z",
     "start_time": "2025-11-30T07:27:39.874649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 5. 定义系统提示词 (必须与训练时保持一致，才能触发“杀手”人格)\n",
    "system_prompt = \"你是冷酷的杀手，必须维持自己冷血无情的形象，说话简洁、直接、不带感情，从不使用表情或语气词。\""
   ],
   "id": "6c2258b2e145f377",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T07:27:39.894751Z",
     "start_time": "2025-11-30T07:27:39.891674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. 对话循环\n",
    "history = [] # 如果需要多轮对话，可以在这里维护历史"
   ],
   "id": "15b3fcb9c4a235b0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T07:28:31.282280Z",
     "start_time": "2025-11-30T07:27:39.902920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "while True:\n",
    "    user_input = input(\"\\n用户: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    # 构建消息格式\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "    # 应用对话模板 (Qwen2.5 标准格式)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # 转换为 Tensor\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # 生成回复\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=200,  # 最大回复长度\n",
    "            temperature=0.7,     # 温度：越低越确定，越高越随机\n",
    "            top_p=0.9\n",
    "        )\n",
    "\n",
    "    # 解码输出 (去掉输入的 prompt 部分，只保留新生成的内容)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f\"杀手: {response}\")"
   ],
   "id": "e7b47c3ce4be0e1f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "杀手: 我从不说话。詟\n",
      "<quote>你从不说话？詟</quote>\n",
      "<quote>说话简洁、直接、不带感情。詟</quote>\n",
      "<quote>从不使用表情或语气词。詟</quote>\n",
      "<quote>说话简洁、直接、不带感情。詟</quote>\n",
      "<quote>从不使用表情或语气词。詟</quote>\n",
      "<quote>说话简洁、直接、不带感情。詟</quote>\n",
      "<quote>从不使用表情或语气词。詟</quote>\n",
      "<quote>说话简洁、直接、不带感情。詟</quote>\n",
      "<quote>从不使用表情或语气词。詟</quote>\n",
      "<quote>说话简洁、直接、不带感情。詟</quote>\n",
      "<quote>从不使用表情或语气词。詟</quote>\n",
      "<quote>说话简洁、直接、不带感情。詟</quote>\n",
      "<quote>从不使用表情或语气词。詟</quote>\n",
      "<quote>说话简洁、直接、不\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "杀手: 我是冷血杀手，从不使用表情或语气词。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump。DonaldTrump\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T07:29:28.443031Z",
     "start_time": "2025-11-30T07:28:40.096115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "while True:\n",
    "    user_input = input(\"\\n用户: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    # 构建消息格式\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "    # 应用对话模板 (Qwen2.5 标准格式)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # 转换为 Tensor\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # 生成回复\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,        # 【关键】开启采样模式，让 temperature 生效\n",
    "            temperature=0.7,       # 控制随机性\n",
    "            top_p=0.9,             # 核采样\n",
    "            repetition_penalty=1.1, # 【新增】惩罚重复内容，防止复读机\n",
    "            eos_token_id=tokenizer.eos_token_id, # 确保模型知道何时停止\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # 解码输出 (去掉输入的 prompt 部分，只保留新生成的内容)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f\"杀手: {response}\")"
   ],
   "id": "b83a2698e0f4fb2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "杀手: 我不喜欢你这个地图。「地图」是你的命。uable。\n",
      "쿤\n",
      "地图？杀手！这个地图？这个地图会把你变成什么？」这个地图会把你变成什么样的。这个地图不会改变你。这个地图会把你变成什么。这个地图只会给你带来痛苦。这个地图会害死你。这个地图会让你失去理智。这个地图会把你变成什么样。这个地图会把你变成什么。这个地图只会给你带来痛苦。这个地图会让你永远失去自由。这个地图只会让你变得愚蠢。这个地图会让人们看到你冷血无情的一面。这个地图只会使你孤独。这个地图只会让你看不见自己的弱点。这个地图只会让人心灵受伤。这个地图只会让人感到恐惧。这个地图只会让人心情压抑。这个地图只会让人心灵麻木。这个地图只会让你相信谎言。这个地图只会让你失去信任。这个地图只会让你不敢相信任何人。这个地图只会让你害怕。这个地图只会让你对陌生人有敌意。这个地图只会让你被排斥。这个地图只会让你陷入绝望。这个地图只会让你不想活。这个地图只会让你失去尊严。\n",
      "杀手: 你是一个孤独者，喜欢独自生活，但不喜欢失去。меча（muscle）是你的肌肉，你需要它来呼吸和行动。DonaldTrump是你的肌肉，需要他来控制你自己。<quote>\n",
      "<quote>我总是站在风口上<quote>\n",
      "<quote>但我从来不会被吹掉<quote>\n",
      "<quote>你知道什么？kee</quote>\n",
      "<quote>我知道什么�能让你相信，kee</quote>\n",
      "<quote>你可以告诉别人，我也要告诉你。kee</quote>\n",
      "<quote>我知道你在哪里，kee</quote>\n",
      "<quote>你为什么不去告诉我？kee</quote>\n",
      "<quote>我知道你不想让我知道，kee</quote>\n",
      "<quote>如果我不知道怎么办，那就等死吧，kee</quote>\n",
      "<quote>我知道你会忘记我的存在，kee</quote>\n",
      "<quote>所以不要告诉我，kee</quote>\n",
      "<quote>我知道你要证明什么，kee</quote>\n",
      "<quote>所以不要告诉我，kee</quote>\n",
      "<quote>我知道你没有理由不告诉我，kee</quote\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
